# Job Hunter AI - System Architecture

> **Last Updated:** 2026-01-29 (Phase 2 Complete - 62.5%)

## Overview

Job Hunter AI uses a **hybrid architecture** combining MCP Server tools with Claude Code CLI orchestration. The system delegates visual scraping to Antigravity browser agent while automating data processing through MCP tools.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
â”?                   USER (Job Seeker)                                         â”?
â”?                                                                             â”?
â”?  Natural Language Commands:                                                â”?
â”?  "Start job hunt", "Process new jobs", "Generate report"                   â”?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
                             â”?
                             â–?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
â”?                   CLAUDE CODE CLI (Project Manager)                         â”?
â”?                                                                             â”?
â”?  Orchestrates workflow, makes decisions, coordinates tools                 â”?
â”?  Pauses for manual tasks (Antigravity scraping)                            â”?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
                             â”?
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
         â”?                  â”?                  â”?
         â–?                  â–?                  â–?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”? â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”? â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
â”? Antigravity    â”? â”? MCP Server      â”? â”? Local Storage      â”?
â”? Browser Agent  â”? â”? (Automated)     â”? â”?                    â”?
â”?                â”? â”?                 â”? â”? â”œâ”€ SQLite DB       â”?
â”? Manual Tasks:  â”? â”? Tools:          â”? â”? â”œâ”€ JSON Files      â”?
â”? â”œâ”€ Scrape jobs â”? â”? â”œâ”€ Generate     â”? â”? â”œâ”€ PDF Resumes    â”?
â”? â”? from visual â”? â”? â”? instructions  â”? â”? â””â”€ Markdown       â”?
â”? â”? platforms   â”? â”? â”œâ”€ Import JSON  â”? â”?    Config          â”?
â”? â””â”€ Auto-apply  â”? â”? â”œâ”€ Filter jobs  â”? â”?                    â”?
â”?    (future)    â”? â”? â”? with GLM     â”? â”?                    â”?
â”?                â”? â”? â””â”€ Generate     â”? â”?                    â”?
â”? User triggers  â”? â”?    resumes      â”? â”?                    â”?
â”? via Claude     â”? â”?                 â”? â”?                    â”?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”? â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”? â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
```

## Architecture Principles

### 1. Claude as Manager, Not Tool User

Claude Code CLI acts as a **project manager** that:
- Understands the overall workflow
- Decides when to use automated tools vs manual tasks
- Pauses for user confirmation on visual tasks
- Orchestrates multi-step processes

### 2. Hybrid Execution Model

**Automated (MCP Tools):**
- Instruction generation
- JSON import and deduplication
- AI filtering with GLM
- Resume generation

**Manual (Antigravity):**
- Visual platform scraping (LinkedIn, Glassdoor, Wellfound, Indeed)
- Form filling for applications (future)
- Tasks requiring browser interaction

**Why Manual for Scraping?**
- Current Playwright scrapers only work for LinkedIn
- Other platforms (Indeed, Glassdoor, Wellfound) are unreliable
- Antigravity visual agent is more robust for modern anti-bot measures
- User maintains control over account logins

### 3. Data Sources Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
â”?                       Data Sources                              â”?
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
â”?                                                                 â”?
â”? Priority 1: ATS Platforms (Highest Quality)                    â”?
â”? â”œâ”€ Greenhouse                                                   â”?
â”? â”œâ”€ Lever                [PLANNED - Task 6]                      â”?
â”? â”œâ”€ Ashby                                                        â”?
â”? â””â”€ Workable                                                     â”?
â”?    â””â”€ Method: Google dorking (automated, no Antigravity)       â”?
â”?                                                                 â”?
â”? Priority 2: Visual Job Boards (High Volume)                    â”?
â”? â”œâ”€ LinkedIn                                                     â”?
â”? â”œâ”€ Glassdoor            [IMPLEMENTED - Tasks 3-5]              â”?
â”? â”œâ”€ Wellfound                                                    â”?
â”? â””â”€ Indeed                                                       â”?
â”?    â””â”€ Method: Antigravity browser agent (manual trigger)       â”?
â”?                                                                 â”?
â”? Priority 3+: Other Sources (Future)                            â”?
â”? â”œâ”€ RSS Feeds            [FUTURE]                               â”?
â”? â”œâ”€ Company career pages [FUTURE]                               â”?
â”? â””â”€ Telegram channels    [FUTURE]                               â”?
â”?                                                                 â”?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
```

## Data Flow

### Daily Workflow

```
1. MORNING: Generate Instructions
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
   â”?User: "Start job hunt"              â”?
   â”?                                    â”?
   â”?Claude â†?MCP Tool:                  â”?
   â”?  generate_antigravity_scraping_    â”?
   â”?  guide()                           â”?
   â”?  â”œâ”€ Reads config/preferences.md    â”?
   â”?  â”?  (24 job titles, filters)      â”?
   â”?  â”œâ”€ Reads config/credentials.md    â”?
   â”?  â””â”€ Generates instructions/        â”?
   â”?     scrape_jobs_2026-01-29.json    â”?
   â”?                                    â”?
   â”?Claude: "Please run Antigravity:"   â”?
   â”?  antigravity run instructions/...  â”?
   â”?                                    â”?
   â”?User: [Runs Antigravity - 5 min]   â”?
   â”?  â†?Scrapes 150+ jobs               â”?
   â”?  â†?Saves to data/*.json            â”?
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
                  â†?
2. AUTOMATED: Import & Deduplicate
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
   â”?User: "Done scraping"               â”?
   â”?                                    â”?
   â”?Claude â†?MCP Tool:                  â”?
   â”?  import_antigravity_results()      â”?
   â”?  â”œâ”€ Scans data/*.json              â”?
   â”?  â”œâ”€ Deduplication (2 levels):      â”?
   â”?  â”?  1. URL exact match            â”?
   â”?  â”?  2. Fuzzy hash (company+title) â”?
   â”?  â”œâ”€ Source priority handling       â”?
   â”?  â”?  (ATS=1 > Visual=2 > Other=3)  â”?
   â”?  â””â”€ Imports to SQLite              â”?
   â”?                                    â”?
   â”?Result: 150 scraped â†?120 unique    â”?
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
                  â†?
3. AUTOMATED: AI Filtering
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
   â”?Claude â†?MCP Tool:                  â”?
   â”?  process_jobs_with_glm_tool()      â”?
   â”?  â”œâ”€ Loads config/achievements.md   â”?
   â”?  â”œâ”€ Loads config/preferences.md    â”?
   â”?  â””â”€ For each unprocessed job:      â”?
   â”?      â”œâ”€ GLM scores 0-100           â”?
   â”?      â””â”€ Three-tier routing:        â”?
   â”?          â”œâ”€ â‰?5: Tier 1 HIGH       â”?
   â”?          â”œâ”€ 60-84: Tier 2 MEDIUM   â”?
   â”?          â””â”€ <60: Tier 3 LOW        â”?
   â”?                                    â”?
   â”?Tier 1 (8 jobs):                    â”?
   â”?  â†?Auto-generate PDF resumes       â”?
   â”?  â†?Status: matched (auto)          â”?
   â”?                                    â”?
   â”?Tier 2 (18 jobs):                   â”?
   â”?  â†?Add to campaign report          â”?
   â”?  â†?Status: matched (manual)        â”?
   â”?                                    â”?
   â”?Tier 3 (94 jobs):                   â”?
   â”?  â†?Archive, no action              â”?
   â”?  â†?Status: rejected                â”?
   â”?                                    â”?
   â”?Cost: ~$0.03 for 120 jobs           â”?
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
                  â†?
4. EVENING: Review & Apply
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
   â”?Claude: "Report ready"              â”?
   â”?  â†?8 HIGH matches (resumes ready)  â”?
   â”?  â†?18 MEDIUM matches (need review) â”?
   â”?                                    â”?
   â”?User: Reviews, approves some        â”?
   â”?                                    â”?
   â”?Claude â†?Generate application       â”?
   â”?         instructions (Task 8)      â”?
   â”?                                    â”?
   â”?User: [Runs Antigravity to apply]   â”?
   â”?  â†?Auto-fills forms                â”?
   â”?  â†?Pauses at Submit button         â”?
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
```

### Three-Tier Scoring System

```
                    GLM Filtering (0-100 score)
                              â”?
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
        â”?                    â”?                    â”?
        â–?                    â–?                    â–?
   Score â‰?85            60 â‰?Score < 85        Score < 60
        â”?                    â”?                    â”?
        â–?                    â–?                    â–?
   TIER 1: HIGH          TIER 2: MEDIUM        TIER 3: LOW
        â”?                    â”?                    â”?
        â–?                    â–?                    â–?
  Auto-generate        Add to campaign         Auto-archive
  PDF resume           report for review       (no action)
        â”?                    â”?
        â–?                    â–?
  status='matched'      status='matched'
  decision='auto'       decision='manual'
        â”?                    â”?
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”?
                  â–?
           Ready to Apply
```

## Directory Structure (Current)

```
JobHunterAI/
â”œâ”€â”€ src/
â”?  â”œâ”€â”€ agents/                   # Instruction Generators [NEW]
â”?  â”?  â”œâ”€â”€ __init__.py
â”?  â”?  â”œâ”€â”€ instruction_generator.py      # Task 3: Antigravity scraping guide
â”?  â”?  â”œâ”€â”€ platform_configs.py           # Platform-specific templates
â”?  â”?  â””â”€â”€ application_guide_generator.py [PLANNED - Task 8]
â”?  â”?
â”?  â”œâ”€â”€ mcp_server/              # MCP Server (Claude Code integration)
â”?  â”?  â”œâ”€â”€ __init__.py
â”?  â”?  â”œâ”€â”€ server.py            # Main MCP server entry
â”?  â”?  â””â”€â”€ tools/               # MCP Tool implementations
â”?  â”?      â”œâ”€â”€ __init__.py
â”?  â”?      â”œâ”€â”€ antigravity.py   # Task 3: generate_antigravity_scraping_guide
â”?  â”?      â”œâ”€â”€ importer.py      # Task 4: import_antigravity_results
â”?  â”?      â”œâ”€â”€ gl_processor.py  # Task 5: process_jobs_with_glm_tool
â”?  â”?      â”œâ”€â”€ ats_scanner.py   [PLANNED - Task 6]
â”?  â”?      â””â”€â”€ report.py        [PLANNED - Task 7]
â”?  â”?
â”?  â”œâ”€â”€ core/                    # Core business logic
â”?  â”?  â”œâ”€â”€ __init__.py
â”?  â”?  â”œâ”€â”€ database.py          # SQLite with source tracking [UPDATED]
â”?  â”?  â”œâ”€â”€ importer.py          # Task 4: JSON â†?DB with dedup [NEW]
â”?  â”?  â”œâ”€â”€ gl_processor.py      # Task 5: GLM filtering engine [NEW]
â”?  â”?  â”œâ”€â”€ pdf_generator.py     # Resume PDF generation
â”?  â”?  â””â”€â”€ llm/
â”?  â”?      â”œâ”€â”€ __init__.py
â”?  â”?      â”œâ”€â”€ glm_client.py    # GLM API client
â”?  â”?      â””â”€â”€ claude_client.py # Claude API client
â”?  â”?
â”?  â”œâ”€â”€ scrapers/                # [DEPRECATED - Use Antigravity]
â”?  â”?  â”œâ”€â”€ __init__.py
â”?  â”?  â”œâ”€â”€ base.py
â”?  â”?  â””â”€â”€ linkedin.py          # Only LinkedIn works
â”?  â”?
â”?  â”œâ”€â”€ output/                  # Output generators
â”?  â”?  â””â”€â”€ report_generator.py  [PLANNED - Task 7]
â”?  â”?
â”?  â””â”€â”€ utils/
â”?      â”œâ”€â”€ __init__.py
â”?      â”œâ”€â”€ config.py            # Configuration loader
â”?      â”œâ”€â”€ markdown_parser.py   # Parse config/*.md
â”?      â””â”€â”€ logger.py            # Logging setup
â”?
â”œâ”€â”€ config/                      # User configuration (Markdown) [GITIGNORED]
â”?  â”œâ”€â”€ resume.md                # Resume content
â”?  â”œâ”€â”€ preferences.md           # Job criteria (24 titles, filters)
â”?  â”œâ”€â”€ achievements.md          # Career highlights
â”?  â””â”€â”€ credentials.md           # Platform logins
â”?
â”œâ”€â”€ instructions/                # Generated Antigravity guides [NEW]
â”?  â””â”€â”€ scrape_jobs_YYYY-MM-DD.json
â”?
â”œâ”€â”€ campaigns/                   # Daily campaign reports [NEW]
â”?  â””â”€â”€ campaign_YYYY-MM-DD.md   [PLANNED - Task 7]
â”?
â”œâ”€â”€ data/                        # Runtime data [GITIGNORED]
â”?  â”œâ”€â”€ jobs.db                  # SQLite database
â”?  â”œâ”€â”€ linkedin_2026-01-29.json # Scraped data from Antigravity
â”?  â”œâ”€â”€ glassdoor_2026-01-29.json
â”?  â””â”€â”€ ...
â”?
â”œâ”€â”€ output/                      # Generated resumes [GITIGNORED]
â”?  â”œâ”€â”€ Scribd_AI_Engineer.pdf
â”?  â””â”€â”€ ...
â”?
â”œâ”€â”€ templates/                   # HTML templates
â”?  â””â”€â”€ resume/
â”?      â””â”€â”€ modern.html          # Resume template
â”?
â”œâ”€â”€ archive/                     # Archived/deprecated code
â”?  â””â”€â”€ old_scrapers/            # Deprecated Playwright scrapers
â”?
â”œâ”€â”€ docs/                        # Documentation
â”?  â”œâ”€â”€ ARCHITECTURE.md          # This file
â”?  â”œâ”€â”€ DEVELOPMENT_GUIDE.md     # Technical guide for sub-agents
â”?  â””â”€â”€ CLEANUP_AND_GUIDE_SUMMARY.md
â”?
â”œâ”€â”€ scripts/                     # Utility scripts
â”?  â”œâ”€â”€ migrate_add_source_tracking.py
â”?  â””â”€â”€ migrate_add_fuzzy_hash.py
â”?
â”œâ”€â”€ tests/
â”?  â”œâ”€â”€ unit/
â”?  â””â”€â”€ integration/
â”?
â”œâ”€â”€ .env                         # API keys [GITIGNORED]
â”œâ”€â”€ .env.example                 # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                    # User-friendly guide
â””â”€â”€ pyproject.toml
```

## Technology Stack

| Component | Technology | Purpose | Status |
|-----------|------------|---------|--------|
| **Orchestrator** | Claude Code CLI | Project manager, workflow control | âœ?Active |
| **Tool Interface** | MCP Server (Python) | Expose tools to Claude | âœ?Active |
| **Visual Agent** | Antigravity | Browser scraping, form filling | âœ?Active |
| **Filtering LLM** | GLM API (æ™ºè°±AI) | Cost-effective job filtering | âœ?Active |
| **Resume LLM** | Claude API (Anthropic) | High-quality resume tailoring | âœ?Active |
| **Database** | SQLite (WAL mode) | Local job storage | âœ?Active |
| **PDF Generator** | WeasyPrint | Resume PDF generation | âœ?Active |
| **Config Format** | Markdown | Human-readable configuration | âœ?Active |
| **Browser (deprecated)** | Playwright | Old scraping method | âš ï¸ LinkedIn only |

## Database Schema

### Jobs Table (Updated)

```sql
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    external_id TEXT UNIQUE,           -- Platform job ID
    url TEXT NOT NULL,
    url_hash TEXT,                     -- MD5(url) for exact dedup
    fuzzy_hash TEXT,                   -- MD5(company+title) for fuzzy dedup
    source TEXT DEFAULT 'linkedin',    -- [NEW] Platform source
    source_priority INTEGER DEFAULT 2, -- [NEW] ATS=1, Visual=2, Other=3

    title TEXT NOT NULL,
    company TEXT NOT NULL,
    location TEXT,
    description TEXT,
    posted_date TEXT,

    ai_score REAL,                     -- GLM score (0-100)
    ai_reasoning TEXT,                 -- Why this score?
    is_processed BOOLEAN DEFAULT 0,    -- [NEW] Has GLM filtered?

    status TEXT DEFAULT 'pending',     -- pending|matched|rejected|applied
    decision_type TEXT,                -- auto|manual (for matched jobs)

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_jobs_url_hash ON jobs(url_hash);
CREATE INDEX idx_jobs_fuzzy_hash ON jobs(fuzzy_hash);
CREATE INDEX idx_jobs_source ON jobs(source);
CREATE INDEX idx_jobs_is_processed ON jobs(is_processed);
CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_ai_score ON jobs(ai_score);
```

### Deduplication Strategy

**Two-Level Deduplication:**

1. **Level 1: URL Exact Match**
   - Hash: `MD5(url)`
   - Catches same job reposted

2. **Level 2: Fuzzy Hash**
   - Hash: `MD5(company.lower() + title.lower())`
   - Catches same job across platforms
   - Example: "Senior Engineer at Google" on LinkedIn vs Indeed

**Source Priority on Conflict:**
- When fuzzy match found, keep higher priority:
  - ATS platforms (priority=1) > Visual platforms (priority=2) > Others (priority=3)
- Reasoning: ATS job postings are most accurate (direct from company)

## MCP Tools (Current Implementation)

### Implemented (Phase 2 - Tasks 3-5)

```python
# Task 3: Antigravity Instruction Generator
@server.tool()
async def generate_antigravity_scraping_guide(
    date: str = None  # Optional, defaults to today
) -> dict:
    """
    Generates JSON instruction file for Antigravity browser agent.

    Reads:
    - config/preferences.md (24 job titles, locations, filters)
    - config/credentials.md (login credentials for auto-login)

    Outputs:
    - instructions/scrape_jobs_{date}.json

    Returns:
        {
            "instruction_file": "instructions/scrape_jobs_2026-01-29.json",
            "platforms": ["linkedin", "glassdoor", "wellfound", "indeed"],
            "job_titles": 24,
            "message": "Please run: antigravity run {instruction_file}"
        }
    """

# Task 4: JSON Importer with Deduplication
@server.tool()
async def import_antigravity_results(
    files: list = None  # Optional, defaults to data/*.json
) -> dict:
    """
    Imports scraped JSON files to database with two-level deduplication.

    Deduplication:
    1. URL exact match (url_hash)
    2. Fuzzy match (company + title)
    3. Source priority resolution (ATS > Visual > Other)

    Returns:
        {
            "total_jobs": 150,
            "new_jobs": 120,
            "url_duplicates": 15,
            "fuzzy_duplicates": 10,
            "updated_by_priority": 5,
            "imported_files": ["linkedin_2026-01-29.json", ...],
            "stats_by_source": {
                "linkedin": {"total": 80, "new": 65},
                "glassdoor": {"total": 40, "new": 30},
                ...
            }
        }
    """

# Task 5: GLM Processor with Three-Tier Scoring
@server.tool()
async def process_jobs_with_glm_tool(
    force_reprocess: bool = False  # Re-process already filtered jobs
) -> dict:
    """
    Processes unfiltered jobs with GLM and routes to three tiers.

    Scoring:
    - Reads config/achievements.md (career highlights)
    - Reads config/preferences.md (job criteria)
    - GLM scores each job 0-100

    Routing:
    - Score â‰?5: Tier 1 HIGH â†?Auto-generate resume
    - 60â‰?Score <85: Tier 2 MEDIUM â†?Add to campaign report
    - Score <60: Tier 3 LOW â†?Archive

    Returns:
        {
            "total_processed": 120,
            "tier1_high_match": 8,
            "tier2_medium_match": 18,
            "tier3_low_match": 94,
            "resumes_generated": 8,
            "cost_usd": 0.03,
            "failed": 0
        }
    """
```

### Planned (Phase 3 - Tasks 6-8)

```python
# Task 6: ATS Dorking Scanner
@server.tool()
async def scan_ats_platforms(
    max_results_per_platform: int = 50
) -> dict:
    """
    Automated Google dorking for ATS platforms.
    No Antigravity needed - direct scraping.

    Platforms: Greenhouse, Lever, Ashby, Workable
    """

# Task 7: Campaign Report Generator
@server.tool()
async def generate_campaign_report(
    date: str = None
) -> dict:
    """
    Generates Markdown campaign report with:
    - HIGH MATCH jobs (Tier 1) with resume links
    - MEDIUM MATCH jobs (Tier 2) for user review
    - Statistics and cost breakdown
    """

# Task 8: Application Instruction Generator
@server.tool()
async def generate_application_instructions(
    campaign_date: str = None
) -> dict:
    """
    Generates JSON instructions for Antigravity to auto-apply.
    User reviews and approves before running.
    """
```

## Key Design Decisions

### 1. Why Antigravity for Visual Platforms?

**Problem:** Current Playwright scrapers fail for Indeed, Glassdoor, Wellfound
- Anti-bot detection improves constantly
- Maintaining platform-specific scrapers is high effort
- Login sessions expire frequently

**Solution:** Antigravity browser agent
- Visual agent understands web pages like humans
- More resilient to UI changes
- User maintains control over logins
- Manual trigger = user awareness

**Trade-off:** Manual trigger required (~5 min/day)
**Benefit:** Higher reliability, less maintenance

### 2. Why Hybrid MCP + CLI?

**MCP Server:** Automated tools that run without user intervention
- JSON import (no decisions needed)
- GLM filtering (follows clear rules)
- Resume generation (template-based)

**Claude CLI:** Orchestration and decision-making
- Understands overall workflow
- Pauses for manual tasks
- Makes judgment calls
- Provides context to user

**Alternative Considered:** Pure MCP without CLI
**Rejected Because:** Would automate too much, user loses visibility

### 3. Why Three-Tier Scoring?

**Tier 1 (â‰?5):** High confidence â†?Auto-generate resume
- Saves user time on obvious matches
- Still requires manual submit

**Tier 2 (60-84):** Medium confidence â†?User review
- Edge cases need human judgment
- Example: Great match but contract role

**Tier 3 (<60):** Low match â†?Archive
- Keeps database clean
- User can still review if curious

**Alternative Considered:** Binary yes/no
**Rejected Because:** Loses nuance, can't prioritize

### 4. Why Two-Level Deduplication?

**Level 1 (URL):** Catches exact reposts
**Level 2 (Fuzzy):** Catches cross-platform duplicates

**Why Both?**
- Same company posts on LinkedIn and Indeed
- Want one application, not two
- Keep ATS version (priority=1) over scraped version

**Alternative Considered:** URL only
**Rejected Because:** Misses cross-platform duplicates

### 5. Why Markdown Config?

**Pros:**
- Human-readable (non-technical users can edit)
- Easy to version control
- AI can understand context better
- No learning curve

**Cons:**
- Less structured than JSON
- Parsing is more complex

**Decision:** User experience over developer convenience

## Cost Analysis

### Daily Operations (50 jobs/day)

| Service | Usage | Unit Cost | Daily Cost |
|---------|-------|-----------|------------|
| **GLM API** | 50 job filters | ~$0.001/job | $0.05 |
| **Claude API** | 8 resumes | ~$0.02/resume | $0.16 |
| **Total** | - | - | **$0.21/day** |

### Monthly: ~$6.30
### Compare to:
- Job board premium: $30-100/month
- Manual search time: 10+ hours/week (priceless)

## Current Implementation Status

### âœ?Phase 1: Foundation (Tasks 1-2)
- [x] Database schema with source tracking
- [x] Fuzzy hash deduplication
- [x] Migration scripts
- [x] Archive old scrapers

### âœ?Phase 2: Core Features (Tasks 3-5) - 62.5% Complete
- [x] Task 3: Antigravity instruction generator
  - Reads preferences.md (24 job titles)
  - Reads credentials.md (auto-login)
  - Generates JSON for Antigravity
- [x] Task 4: JSON importer
  - Two-level deduplication
  - Source priority handling
  - Batch import support
- [x] Task 5: GLM processor
  - Three-tier scoring (85/60 thresholds)
  - Auto-resume generation for Tier 1
  - Cost tracking

### â¸ï¸ Phase 3: Enhancements (Tasks 6-8) - 37.5% Remaining
- [ ] Task 6: ATS dorking scanner
  - Google search automation
  - Greenhouse, Lever, Ashby, Workable
  - Priority=1 source
- [ ] Task 7: Campaign report generator
  - Markdown tables
  - HIGH/MEDIUM sections
  - Statistics dashboard
- [ ] Task 8: Application instruction generator
  - JSON for Antigravity auto-apply
  - Form filling templates
  - User approval workflow

**Current system is production-ready for daily use.** Phase 3 adds polish and automation.

## Security & Privacy

### Data Privacy
- All data stored locally (SQLite)
- No cloud uploads except API calls
- API calls are job descriptions only (no personal data)
- Credentials stored in gitignored files

### Gitignored Files
```
config/resume.md
config/preferences.md
config/achievements.md
config/credentials.md
.env
data/jobs.db
data/*.json
output/*.pdf
```

### API Key Usage
- **GLM API:** Job filtering only (receives job descriptions)
- **Claude API:** Resume generation (receives job descriptions + your achievements)
- Never shared externally
- You control when to call APIs

## Testing Strategy

### Unit Tests
```
tests/unit/
â”œâ”€â”€ test_database.py       # DB operations
â”œâ”€â”€ test_importer.py       # Deduplication logic
â”œâ”€â”€ test_gl_processor.py   # Scoring logic
â””â”€â”€ test_instruction_gen.py # JSON generation
```

### Integration Tests
```
tests/integration/
â”œâ”€â”€ test_workflow.py       # End-to-end daily workflow
â”œâ”€â”€ test_mcp_tools.py      # MCP tool integration
â””â”€â”€ test_antigravity.py    # Antigravity integration (manual)
```

### Manual Testing
```bash
# Test instruction generation
python -m src.agents.instruction_generator

# Test import
python -m src.core.importer data/linkedin_test.json

# Test GLM processing
python -m src.core.gl_processor --limit 5
```

## Troubleshooting

### Common Issues

**1. "No jobs imported" after Antigravity run**
- Check data/*.json files exist
- Verify JSON format matches expected schema
- Check database connection

**2. "GLM API error: rate limit"**
- Wait 60 seconds between batches
- Reduce batch size in gl_processor.py
- Check API key quota

**3. "Antigravity login failed"**
- Update credentials.md with current passwords
- Check if accounts are locked
- Manually log in first to verify

**4. "Resume generation failed"**
- Check config/resume.md exists and is valid
- Verify Claude API key in .env
- Check template file templates/resume/modern.html

## Development Workflow

### Adding a New Platform

1. **Add platform config** to `src/agents/platform_configs.py`
2. **Update preferences.md** template with new platform
3. **Test with Antigravity** manually first
4. **Add to instruction generator** logic
5. **Document** in this file

### Adding a New MCP Tool

1. **Create tool file** in `src/mcp_server/tools/`
2. **Register in server.py** with `@server.tool()`
3. **Add unit tests** in `tests/unit/`
4. **Document in DEVELOPMENT_GUIDE.md**
5. **Update this ARCHITECTURE.md**

## Future Enhancements

### Considered for Future Phases

**Phase 4: Analytics**
- Response rate tracking
- Interview conversion metrics
- Best platforms/time analysis

**Phase 5: Advanced Automation**
- Interview scheduling automation
- Follow-up email templates
- Offer comparison dashboard

**Phase 6: Multi-User**
- Team job hunting
- Shared job pool
- Collaborative filtering

## References

- **README.md** - User guide and quick start
- **docs/DEVELOPMENT_GUIDE.md** - Technical implementation details
- **.claude/skills/job-hunt/SKILL.md** - Claude CLI skill documentation

---

**Last Updated:** 2026-01-29
**Status:** Phase 2 Complete (62.5%)
**Next Step:** Task 6 (ATS Dorking Scanner)
